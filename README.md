# Snake DQN - Juego de Snake con Aprendizaje por Refuerzo Profundo

<p align="center">
  <img src="assets/snake_dqn_logo.png" alt="Snake DQN Logo" width="200" height="200" />
</p>

Este proyecto implementa el clÃ¡sico juego de Snake utilizando Deep Q-Learning (DQN) para entrenar un agente que aprenda a jugar de forma autÃ³noma. El sistema utiliza tÃ©cnicas avanzadas de aprendizaje por refuerzo para desarrollar estrategias Ã³ptimas de juego.

## ğŸ’« CaracterÃ­sticas Principales

- **ImplementaciÃ³n Completa del Juego**: Desarrollado con PyGame, con mÃºltiples modos de visualizaciÃ³n.
- **Algoritmo DQN Avanzado**: Incluye mejoras como Double DQN, Prioritized Experience Replay y estrategias de exploraciÃ³n adaptativas.
- **VisualizaciÃ³n en Tiempo Real**: Observa cÃ³mo el agente aprende y mejora con el tiempo.
- **Interfaz GrÃ¡fica Mejorada**: DiseÃ±o de "estadio" con efectos visuales personalizables.
- **Sistema de Pathfinding**: Algoritmos A\* y bÃºsqueda de caminos largos para evitar situaciones de bloqueo.
- **AnÃ¡lisis Detallado**: Seguimiento y visualizaciÃ³n de mÃ©tricas de entrenamiento.
- **ConfiguraciÃ³n Persistente**: Guarda tus preferencias visuales entre sesiones.
- **Seguridad Mejorada**: ValidaciÃ³n robusta de datos y manejo avanzado de excepciones.

## ğŸ’» Requisitos TÃ©cnicos

- Python 3.7 o superior
- Dependencias principales:
  - PyGame >= 2.0.0 (motor del juego)
  - PyTorch >= 1.7.0 (framework de aprendizaje profundo)
  - NumPy >= 1.19.0 (procesamiento numÃ©rico)
  - Pandas >= 1.1.0 (anÃ¡lisis de datos)
  - Matplotlib >= 3.3.0 y Seaborn >= 0.11.0 (visualizaciÃ³n)

## ğŸ“ InstalaciÃ³n

### OpciÃ³n 1: InstalaciÃ³n EstÃ¡ndar

1. Clona este repositorio:

   ```bash
   git clone https://github.com/tu-usuario/snake-dqn.git
   cd snake-dqn
   ```

2. Instala las dependencias:
   ```bash
   pip install -r requirements.txt
   ```

### OpciÃ³n 2: Usando un Entorno Virtual (Recomendado)

1. Clona el repositorio y crea un entorno virtual:

   ```bash
   git clone https://github.com/tu-usuario/snake-dqn.git
   cd snake-dqn
   python -m venv venv
   ```

2. Activa el entorno virtual:

   - En Windows:
     ```bash
     venv\Scripts\activate
     ```
   - En macOS/Linux:
     ```bash
     source venv/bin/activate
     ```

3. Instala las dependencias:
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ® Uso

### Iniciar el Entrenamiento

Para comenzar el entrenamiento del agente:

```bash
python main.py
```

Al iniciar, se mostrarÃ¡ una pantalla de configuraciÃ³n donde puedes seleccionar las opciones visuales:

- **Modo Animado**: VisualizaciÃ³n completa con efectos grÃ¡ficos.
- **Modo Simple**: Renderizado bÃ¡sico para mayor rendimiento.

### Controles Durante el Entrenamiento

- **V**: Alterna entre modo visual animado y simple.
- **P**: Activa/desactiva el sistema de pathfinding.
- **S**: Cambia el tamaÃ±o de la ventana (pequeÃ±o, mediano, grande).

### Limpiar Archivos Temporales

Para eliminar archivos de cachÃ© y temporales:

```bash
python clean.py
```

## ğŸ“Š Seguimiento del Entrenamiento

Durante el entrenamiento, el sistema genera:

- **GrÃ¡ficos de Progreso**: VisualizaciÃ³n de puntuaciones y recompensas.
- **Archivos CSV**: Datos detallados para anÃ¡lisis posterior.
- **Modelos Guardados**: Puntos de control del entrenamiento.

Los resultados se guardan en las carpetas `plots/` y `results/`.

## ğŸ“‚ Estructura del Proyecto

```
snake_dqn/
â”œâ”€â”€ assets/            # Recursos grÃ¡ficos y fuentes
â”œâ”€â”€ docs/              # DocumentaciÃ³n adicional
â”œâ”€â”€ model_Model/       # Modelos guardados
â”œâ”€â”€ plots/             # GrÃ¡ficos generados
â”œâ”€â”€ results/           # Resultados y anÃ¡lisis
â”œâ”€â”€ src/               # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ agent.py       # Agente de aprendizaje
â”‚   â”œâ”€â”€ game.py        # ImplementaciÃ³n del juego
â”‚   â”œâ”€â”€ model.py       # Arquitectura de la red neuronal
â”‚   â””â”€â”€ start_screen.py # Pantalla de inicio
â”œâ”€â”€ utils/             # Utilidades y herramientas
â”‚   â”œâ”€â”€ advanced_pathfinding.py  # Algoritmos de bÃºsqueda
â”‚   â”œâ”€â”€ config.py                # ConfiguraciÃ³n y parÃ¡metros
â”‚   â”œâ”€â”€ config_manager.py        # GestiÃ³n de configuraciÃ³n
â”‚   â”œâ”€â”€ efficient_memory.py      # GestiÃ³n optimizada de memoria
â”‚   â”œâ”€â”€ evaluation.py            # Herramientas de evaluaciÃ³n
â”‚   â”œâ”€â”€ helper.py                # Funciones auxiliares
â”‚   â””â”€â”€ validation.py            # ValidaciÃ³n de datos y seguridad
â”œâ”€â”€ clean.py           # Script para limpiar archivos temporales
â”œâ”€â”€ inspection.py       # Herramientas de inspecciÃ³n y anÃ¡lisis
â”œâ”€â”€ main.py            # Punto de entrada principal
â”œâ”€â”€ requirements.txt    # Dependencias del proyecto
â””â”€â”€ README.md          # Este archivo
```

## âš™ï¸ ConfiguraciÃ³n

### ConfiguraciÃ³n Visual

La configuraciÃ³n visual se puede ajustar a travÃ©s de la pantalla de inicio y se guarda en `config.json` para futuras sesiones.

### ParÃ¡metros del Sistema

Puedes modificar los parÃ¡metros del juego y del entrenamiento en `utils/config.py`:

- **ParÃ¡metros de Juego**: TamaÃ±o de bloque, velocidad, dimensiones.
- **ParÃ¡metros Visuales**: Efectos, colores, animaciones.
- **HiperparÃ¡metros de DQN**: Tasa de aprendizaje, factor de descuento, tamaÃ±o de lote.
- **ParÃ¡metros de ExploraciÃ³n**: Temperatura, decaimiento, fases de exploraciÃ³n.
- **GestiÃ³n de Memoria**: TamaÃ±o del bÃºfer, umbrales de poda.

## ğŸ’¡ Algoritmos Implementados

### Deep Q-Network (DQN)

El proyecto implementa varias mejoras sobre el algoritmo DQN bÃ¡sico:

- **Double DQN**: Reduce la sobreestimaciÃ³n de valores Q utilizando una red objetivo.
- **Prioritized Experience Replay**: Muestrea experiencias mÃ¡s importantes con mayor frecuencia.
- **ExploraciÃ³n Adaptativa**: Ajusta dinÃ¡micamente la exploraciÃ³n segÃºn el progreso.

### Pathfinding

- **Algoritmo A\***: Encuentra el camino mÃ¡s corto hacia la comida.
- **BÃºsqueda de Caminos Largos**: Evita situaciones de bloqueo en serpientes largas.
- **AnÃ¡lisis de Espacio Libre**: Toma decisiones estratÃ©gicas basadas en el espacio disponible.

## ğŸ“ˆ Rendimiento

El sistema incluye herramientas para monitorear y analizar el rendimiento:

- **Seguimiento de MÃ©tricas**: PuntuaciÃ³n, longitud, recompensa, pÃ©rdida.
- **Alertas AutomÃ¡ticas**: Detecta problemas potenciales durante el entrenamiento.
- **EvaluaciÃ³n PeriÃ³dica**: Prueba el rendimiento del agente en escenarios controlados.

## ğŸ“ DocumentaciÃ³n Adicional

Para mÃ¡s detalles sobre la arquitectura y el diseÃ±o del sistema, consulta los archivos en la carpeta `docs/`:

- [Arquitectura del Sistema](docs/architecture.md)
- [Manual de Usuario](docs/user_manual.md)
- [Mejoras de Seguridad](docs/security_improvements.md)
- [ImplementaciÃ³n de Seguridad](docs/security_implementation.md)

## ğŸ’¬ Contribuciones

Las contribuciones son bienvenidas. Si deseas contribuir:

1. Haz un fork del repositorio
2. Crea una rama para tu funcionalidad (`git checkout -b feature/nueva-funcionalidad`)
3. Realiza tus cambios y haz commit (`git commit -m 'AÃ±adir nueva funcionalidad'`)
4. Sube los cambios a tu fork (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ” Seguridad

El proyecto implementa varias capas de seguridad para garantizar la robustez y estabilidad:

- **ValidaciÃ³n de Entrada**: VerificaciÃ³n exhaustiva de todos los datos externos.
- **Manejo de Excepciones**: RecuperaciÃ³n elegante de errores en Ã¡reas crÃ­ticas.
- **Registro de Seguridad**: Seguimiento detallado de eventos y errores.
- **ValidaciÃ³n de Recursos**: VerificaciÃ³n de integridad de archivos cargados.
- **ProtecciÃ³n contra Datos Malformados**: PrevenciÃ³n de fallos por datos corruptos.

Para mÃ¡s detalles, consulta la [documentaciÃ³n de seguridad](docs/security_implementation.md).

## ğŸ”’ Licencia

Este proyecto estÃ¡ licenciado bajo la [Licencia MIT](LICENSE).

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado como parte de un proyecto de investigaciÃ³n en aprendizaje por refuerzo.

---

<p align="center">
  <i>"La inteligencia artificial es la nueva electricidad." - Andrew Ng</i>
</p>
